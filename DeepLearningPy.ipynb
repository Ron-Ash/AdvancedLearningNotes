{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning With PyTorch\n",
    "**Deep Learning** is a subset of *Machine Learning* where the fundemental structure is a network of inputs, ($\\geq 1$) hidden layers, and outputs. The original intuition of deep learning was modelling how the human brain learns; through inter-connected neurons. One of the most popular deep learning frameworks is ***Pytorch*** ,`torch`, sharing similarities with the `numpy` library (instead uses tensors `torch.tensor([[1, 2, 3], [4, 5, 6]])` instead of arrays/matrices).\n",
    "- Element-wise operations ($X_{a\\times b}\\cdot Y_{a\\times b}$):`x + y` (addition), `x - y` (subtraction), `x * y` (multiplication), `torch.div(x,y)` (division)\n",
    "- Matrix operations ($X_{a\\times b}\\cdot Y_{b\\times c}$): `x @ y` (multiplication), `torch.linalg.inv(x)` (matrix inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (NN)\n",
    "### OOP deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# OOP model to replace:\n",
    "# net = nn.Sequential(nn.Linear(9,16), \n",
    "#               nn.ReLU(), \n",
    "#               nn.Linear(16,8), \n",
    "#               nn.ReLU(), \n",
    "#               nn.Linear(8,1), \n",
    "#               nn.Sigmoid())\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self): # network definition\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(9,16)\n",
    "        self.fc2 = nn.Linear(16,8)\n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "\n",
    "    def forward(self, x): # forward pass\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "\n",
    "## solutions for unstable (vanishing/exploding) gradients:\n",
    "class Net2(Net):\n",
    "    # Batch normalization: (normalize the layers' outputs, scale and shift normalized outputs using learned parameters)\n",
    "    # - model learns optimal inputs distribution for each layer\n",
    "    #   - faster loss decrease\n",
    "    #   - helps against unstable gradients\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        self._weight_initialization()\n",
    "\n",
    "    # Proper weight initialization: (dependent on activation function)\n",
    "    # - ensures variance of layer inputs = variance of layer outputs\n",
    "    # - ensures variance of gradients is the same before and after a layer\n",
    "    def _weight_initialization(self):\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    # Activation functions with non-zero gradients for negative values:(ELU, etc.)\n",
    "    # - help against dying neurons (become 0 and never change after)\n",
    "    # - average output > 0 and thereby help against vanishing gradients\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.elu(x)\n",
    "        # = nn.functional.leaky_relu(x, negative_slope=0.05) # ReLU where slope for x < 0 is != 0\n",
    "        x = self.fc3(x)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.data = df.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, indx):\n",
    "        features = self.data[indx, :-1]\n",
    "        label = self.data[indx, -1]\n",
    "        return features, label\n",
    "    \n",
    "train_dataset = CsvDataset('train.csv')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.95) # stochastic gradient descent\n",
    "#         = optim.Adagrad(net.parameters(), lr=0.01) # adaptive gradient descent (different learning rate for each parameter)\n",
    "#         = optim.RMSprop(net.parameters(), lr=0.01) # root mean square propagation (update for each parameter based on the size of previous gradient)\n",
    "#         = optim.Adam(net.parameters(), lr=0.01) # adaptive moment estimation (RMSprop + gradient momentum)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for features, label in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(features)\n",
    "        loss = criterion(outputs, label.view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "test_dataset = CsvDataset('test.csv')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "acc = Accuracy(task=\"binary\")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for features, label in test_dataloader:\n",
    "        outputs = net(features)\n",
    "        pred = (outputs >= 0.5).float()\n",
    "        acc(pred, label.view(-1,1))\n",
    "\n",
    "accuracy = acc.compute()\n",
    "print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Convolutional NN\n",
    "\n",
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Generating more data by applying random transformations to the original images\n",
    "# - increase size and diversity of training set\n",
    "# - improve model robustness\n",
    "# - reduces overfitting\n",
    "# Note that transformation need to be logical to the data and task (i.e. if designed to detect lemons vs. limes, \n",
    "# cannot alter the color as label would not be accurate anymore)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    # ...\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64,64))\n",
    "])\n",
    "train_dataset = ImageFolder(\"./train\", transform=train_transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "image, label = next(iter(train_dataloader))\n",
    "image = image.squeeze().permute(1, 2, 0) # alter dimensions of image to correctly display it\n",
    "\n",
    "# Note that there should be no data augmentation for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normal Convolutional Layers\n",
    "# Slide overlapping filter(s) of parameters over the input, performing convolution (dot-product) at each position resulting in a feature map (one \n",
    "# filter = one feature map); preserving spatial patters from input, and uses fewer parameters than linear layers. Activation functions are then \n",
    "# applied to each feature map (similar to normal NN), and all maps are combined to form an output.\n",
    "# - a frame of 0s can be added to convolutional layer's input to ensure border pixels are treated equally to others.\n",
    "\n",
    "## MaxPooling\n",
    "# Slide non-overlapping window(s) over the input, and at each position retaining only the maximum value (used after convolutional layers to reduce \n",
    "# spatial dimensions).\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes): # network definition\n",
    "        super().__init__()\n",
    "        self.cl1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.cl2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.cl3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.cl4 = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(16*16*64,num_classes)\n",
    "\n",
    "    def forward(self, x): # forward pass\n",
    "        x = self.cl1(x) # 64x64x3 -> 64x64x32\n",
    "        x = nn.functional.elu(x) \n",
    "        x = self.cl3(x) # 64x64x32 -> 32x32x32\n",
    "        x = self.cl2(x) # 32x32x32 -> 32x32x64\n",
    "        x = nn.functional.elu(x)\n",
    "        x = self.cl3(x) # 32x32x64 -> 16x16x64\n",
    "        x = self.cl4(x) # 16x16x64 -> 16384\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
