{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108f40f1",
   "metadata": {},
   "source": [
    "# Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4df9e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import xgboost as xgb\n",
    "\n",
    "# read dataset \n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df.sort_values(by='indexingFeature').reset_index(drop=True)\n",
    "\n",
    "# remove identification features\n",
    "colsToDrop = ['...']\n",
    "df.drop(colsToDrop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93691dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "isNa = df.isna()\n",
    "data = []\n",
    "for column in isNa.columns:\n",
    "    crosstab = pd.crosstab(isNa[column], df.targetFeature).reindex(index=[False, True], columns=[0, 1], fill_value=0)\n",
    "    \n",
    "    # True->0\n",
    "    suppT0 = crosstab.loc[True, 0] # P(T&0)\n",
    "    confT0 = crosstab.loc[True, 0]/(sum(crosstab.loc[True, :])+1) # ~P(0|T)\n",
    "    liftT0 = crosstab.loc[True, 0]/((sum(crosstab.loc[True, :])+1)*(sum(crosstab.loc[:, 0])+1)) # ~P(0|T)/P(0)\n",
    "    # True->1\n",
    "    suppT1 = crosstab.loc[True, 1] # P(T&1)\n",
    "    confT1 = crosstab.loc[True, 1]/(sum(crosstab.loc[True, :])+1) # ~P(1|T)\n",
    "    liftT1 = crosstab.loc[True, 1]/((sum(crosstab.loc[True, :])+1)*(sum(crosstab.loc[:, 1])+1)) # ~P(1|T)/P(1)\n",
    "    # False->0\n",
    "    suppF0 = crosstab.loc[False, 0] # P(F&0)\n",
    "    confF0 = crosstab.loc[False, 0]/(sum(crosstab.loc[False, :])+1) # ~P(0|F)\n",
    "    liftF0 = crosstab.loc[False, 0]/((sum(crosstab.loc[False, :])+1)*(sum(crosstab.loc[:, 0])+1)) # ~P(0|F)/P(0)\n",
    "    # False->1\n",
    "    suppF1 = crosstab.loc[False, 1] # P(F&1)\n",
    "    confF1 = crosstab.loc[False, 1]/(sum(crosstab.loc[False, :])+1) # ~P(1|F)\n",
    "    liftF1 = crosstab.loc[False, 1]/((sum(crosstab.loc[False, :])+1)*(sum(crosstab.loc[:, 1])+1)) # ~P(1|F)/P(1)\n",
    "\n",
    "    fisherStat, fisherP = fisher_exact(crosstab, alternative='two-sided')\n",
    "    chi2Stat, chi2P, chi2Dof, chi2Expected = chi2_contingency(crosstab + 0.5)\n",
    "    naProp = isNa[column].mean()\n",
    "\n",
    "    data.append([naProp, suppT0, confT0, liftT0, suppT1, confT1, liftT1, suppF0, confF0, liftF0, suppF1, confF1, liftF1, fisherP, chi2P])\n",
    "liftAnalysis = pd.DataFrame(data, columns=['naProp', 'suppT0', 'confT0', 'liftT0', 'suppT1', 'confT1', 'liftT1', 'suppF0', 'confF0', 'liftF0', 'suppF1', 'confF1', 'liftF1', 'fisherP', 'chi2P'], index=isNa.columns)\n",
    "plt.figure(figsize=(30, 5))\n",
    "np.log(liftAnalysis.fisherP).plot(label='fisher exact test')\n",
    "np.log(liftAnalysis.chi2P).plot(label='chi squared test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc611c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "isNa = df.isna().add_suffix('_isNA')\n",
    "df = df.join(isNa, how='left')\n",
    "\n",
    "dfTrain = df[(df.indexingFeature < '2024-01-01') & (df.indexingFeature >= '2021-01-01')]\n",
    "trainIndex = dfTrain.index\n",
    "dfTest = df[df.indexingFeature >= '2024-01-01']\n",
    "dfTrain.to_csv('raw_train.csv')\n",
    "dfTest.to_csv('raw_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab371e",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856abaeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class RemoveEmptyFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.9, verbose=True):\n",
    "        self.threshold = threshold\n",
    "        self.emptyFeatures = None\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== RemoveEmptyFeatures ===\")\n",
    "        self.emptyFeatures = (X.isna().mean()).sort_values()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        nonEmptyFeatureNames = self.emptyFeatures[self.emptyFeatures <= self.threshold].index\n",
    "        return X[nonEmptyFeatureNames].copy()\n",
    "\n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31129775",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RemoveColumnPerFrequency(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=1, verbose=True):\n",
    "        self.threshold = threshold\n",
    "        self.featureModes = None\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== RemoveColumnPerFrequency ===\")\n",
    "        self.featureModes = X.mode(dropna=False).iloc[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        newX = X.copy()\n",
    "        XLen = len(X)\n",
    "        for column in self.featureModes.index:\n",
    "            value = self.featureModes[column]\n",
    "            if pd.isna(value):\n",
    "                modeFreq = X[column].isna().sum()\n",
    "            else:\n",
    "                modeFreq = X[column].value_counts()[value]\n",
    "            modeProp = modeFreq/XLen\n",
    "            if modeProp >= self.threshold:\n",
    "                self.print(f\"dropping: {column} [mode proportion {modeProp}]\")\n",
    "                newX.drop(column, axis=1, inplace=True)\n",
    "        return newX\n",
    "    \n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf978f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RemoveDuplicatedColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== RemoveDuplicatedColumns ===\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        newX = X.copy()\n",
    "        columns = list(X.columns)\n",
    "        for i, colI in enumerate(columns):\n",
    "            for j, colJ in enumerate(columns[i+1:]):\n",
    "                if X[colI].equals(X[colJ]) and colJ in newX.columns:\n",
    "                    self.print(f\"dropping: {colJ} [{colI} equals {colJ}]\")\n",
    "                    newX.drop(colJ, axis=1, inplace=True)\n",
    "        return newX\n",
    "\n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2867005",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "class RemoveCorrelatedColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.75, targetFeature='targetFeature', verbose=True):\n",
    "        self.threshold = threshold\n",
    "        self.targetFeature = targetFeature\n",
    "        self.pairWiseCorr = None\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== RemoveCorrelatedColumns ===\")\n",
    "        self.pairWiseCorr = X.select_dtypes(include=['number']).corr()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        newX = X.copy()\n",
    "        columns = list(self.pairWiseCorr.columns)\n",
    "        for i, colI in enumerate(columns):\n",
    "            if colI not in newX.columns or colI==self.targetFeature:\n",
    "                continue\n",
    "            for j, colJ in enumerate(columns[i+1:]):\n",
    "                if colJ not in newX.columns or colJ==self.targetFeature:\n",
    "                    continue\n",
    "                corr = abs(self.pairWiseCorr.loc[colI, colJ])\n",
    "                if corr >= abs(self.threshold):\n",
    "                    colICorr = abs(self.pairWiseCorr.loc[colI,self.targetFeature])\n",
    "                    colJCorr = abs(self.pairWiseCorr.loc[colJ,self.targetFeature])\n",
    "                    if colICorr >= colJCorr:\n",
    "                        self.print(f\"dropping: {colJ} [Corr(i,j)={corr} Corr(i,t)={colICorr}]\")\n",
    "                        newX.drop(colJ, axis=1, inplace=True)\n",
    "                    else:\n",
    "                        self.print(f\"dropping: {colI} [Corr(i,j)={corr} Corr(j,t)={colJCorr}]\")\n",
    "                        newX.drop(colI, axis=1, inplace=True)\n",
    "                        break\n",
    "        return newX\n",
    "\n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35570d9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "dfTrain = pd.read_csv('raw_train.csv', index_col='Unnamed: 0')\n",
    "assert dfTrain.index.is_monotonic_increasing\n",
    "assert dfTrain.extraction_date.is_monotonic_increasing\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('RemoveEmptyFeatures', RemoveEmptyFeatures(threshold=0.9, verbose=True)),\n",
    "    ('RemoveColumnPerFrequency', RemoveColumnPerFrequency(threshold=1, verbose=True)),\n",
    "    ('RemoveDuplicatedColumns', RemoveDuplicatedColumns(verbose=True)),\n",
    "    ('RemoveCorrelatedColumns', RemoveCorrelatedColumns(threshold=0.75, targetFeature='idc_trgt', verbose=True))])\n",
    "\n",
    "dfTrainClean = pipeline.fit_transform(dfTrain, dfTrain.idc_trgt)\n",
    "dfTrainClean.to_csv(\"clean_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50be80c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dfTest = pd.read_csv('raw_test.csv', index_col='Unnamed: 0')\n",
    "dfTestClean = dfTest[dfTrainClean.columns]\n",
    "dfTestClean.to_csv(\"clean_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ce97f",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030609c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class EncodeDateTimeColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, patterns=[r\".*_dt.*\", r\".*_dttm.*\"], indexingFeature='indexingFeature', verbose=True):\n",
    "        self.patterns = patterns\n",
    "        self.indexingFeature = indexingFeature\n",
    "        self.verbose=verbose\n",
    "        self.dateTimeColumns = None\n",
    "        self.columns = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== EncodeDateTimeColumns ===\")\n",
    "        xObjects = X.select_dtypes(include=['object'])\n",
    "        self.dateTimeColumns = [column for column in xObjects.columns if any(re.search(pattern, column) for pattern in self.patterns)]\n",
    "        if self.indexingFeature in self.dateTimeColumns:\n",
    "            self.dateTimeColumns.remove(self.indexingFeature)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        newX = X.copy()\n",
    "        for column in self.dateTimeColumns:\n",
    "            self.print(f\"converting: {column}\")\n",
    "            newX.loc[:, column] = pd.to_datetime(newX[column], errors='coerce')\n",
    "            newX[f\"{column}_month\"] = newX[column].apply(lambda x: x.month if pd.notna(x) else pd.NA).astype(\"Int64\")\n",
    "            newX[f\"{column}_day\"] = newX[column].apply(lambda x: x.day if pd.notna(x) else pd.NA).astype(\"Int64\")\n",
    "            newX[f\"{column}_dayOfWeek\"] = newX[column].apply(lambda x: x.dayofweek if pd.notna(x) else pd.NA).astype(\"Int64\")\n",
    "            self.columns += [f\"{column}_day\", f\"{column}_month\"]\n",
    "            newX.drop(column, axis=1, inplace=True)\n",
    "        return newX\n",
    "\n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7033a11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EncodeLowCardinality(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=50, indexingFeature='indexingFeature', parentDf=None, verbose=True):\n",
    "        self.threshold = threshold\n",
    "        self.indexingFeature = indexingFeature\n",
    "        self.parentDf = parentDf\n",
    "        if self.parentDf is not None:\n",
    "            self.parentDf = self.parentDf.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        self.verbose=verbose\n",
    "        self.lowCardColumns = None\n",
    "        self.columns = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== EncodeLowCardinality ===\")\n",
    "        xObjects = X.select_dtypes(include=['object']).astype(str).map(lambda x: x.strip())\n",
    "        self.lowCardColumns = [column for column in xObjects.columns if xObjects[column].nunique() < self.threshold]\n",
    "        \n",
    "        if self.parentDf is not None:\n",
    "            self.lowCardColumns = [column for column in xObjects.columns if self.parentDf[column].nunique() < self.threshold]\n",
    "            \n",
    "        if self.indexingFeature in self.lowCardColumns:\n",
    "            self.lowCardColumns.remove(self.indexingFeature)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        newX = X.copy().map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        for column in self.lowCardColumns:\n",
    "            self.print(f\"converting: {column} [{newX[column].nunique()} unique values]\")\n",
    "        oneHotColumns = pd.get_dummies(newX[self.lowCardColumns]).astype(int)\n",
    "        \n",
    "        if self.parentDf is not None:\n",
    "            parentOneHotColumns = pd.get_dummies(self.parentDf[self.lowCardColumns]).astype(str)\n",
    "            oneHotColumnsNames = set(oneHotColumns.columns)\n",
    "            parentOneHotColumnsNames = set(parentOneHotColumns.columns)\n",
    "            oneHotColumns = oneHotColumns[list(oneHotColumnsNames & parentOneHotColumnsNames)]\n",
    "            for column in list(parentOneHotColumnsNames - oneHotColumnsNames):\n",
    "                oneHotColumns[column] = 0\n",
    "        self.columns = list(oneHotColumns.columns)\n",
    "\n",
    "        newX = newX.join(oneHotColumns, how='inner')\n",
    "        newX.drop(self.lowCardColumns, axis=1, inplace=True)\n",
    "        return newX\n",
    "\n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e19699",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EncodeHighCardinality(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=50,  targetFeature='targetFeature', indexingFeature='indexingFeature', parentDf=None, verbose=True):\n",
    "        self.threshold = threshold\n",
    "        self.targetFeature = targetFeature\n",
    "        self.indexingFeature = indexingFeature\n",
    "        self.parentDf = parentDf\n",
    "        if self.parentDf is not None:\n",
    "            self.parentDf = self.parentDf.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        self.verbose=verbose\n",
    "        self.highCardColumns = None\n",
    "        self.columns = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== EncodeHighCardinality ===\")\n",
    "        xObjects = X.select_dtypes(include=['object']).astype(str).map(lambda x: x.strip())\n",
    "        self.highCardColumns = [column for column in xObjects.columns if xObjects[column].nunique() >= self.threshold]\n",
    "\n",
    "        if self.parentDf is not None:\n",
    "            self.lowCardColumns = [column for column in xObjects.columns if self.parentDf[column].nunique() >= self.threshold]\n",
    "        \n",
    "        if self.indexingFeature in self.highCardColumns:\n",
    "            self.highCardColumns.remove(self.indexingFeature)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        newX = X.copy().map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        \n",
    "        xCombined = newX\n",
    "        if self.parentDf is not None:\n",
    "            xCombined = pd.concat([newX, self.parentDf])\n",
    "            \n",
    "        for column in self.highCardColumns:\n",
    "            if column == self.targetFeature:\n",
    "                continue\n",
    "            self.print(f\"converting: {column} [{X[column].nunique()} unique values]\")\n",
    "            target_map = self.target_encoding(xCombined, [column, self.indexingFeature])\n",
    "            mapping = newX[[column, self.indexingFeature]].copy().merge(target_map.reset_index(), on=[column, self.indexingFeature], how='left')\n",
    "            assert mapping['mean'].isna().sum() == newX[column].isna().sum()\n",
    "            newX.loc[:, column] = mapping['mean']\n",
    "            newX[column] = newX[column].astype(float)\n",
    "        self.columns = self.highCardColumns\n",
    "        return newX\n",
    "\n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)\n",
    "\n",
    "    def target_encoding(self, df, columns):\n",
    "        grouping = df.groupby(columns)[self.targetFeature]\n",
    "        sum = grouping.sum().reset_index().groupby(columns[0])[self.targetFeature].cumsum()\n",
    "        count = grouping.count().reset_index().groupby(columns[0])[self.targetFeature].cumsum()\n",
    "        map = grouping.count().reset_index()\n",
    "        map['mean'] = sum/count\n",
    "        map = map[columns + ['mean']].set_index(columns)\n",
    "        return pd.DataFrame(map.groupby(columns[0])['mean'].shift(1).fillna(0), columns=['mean']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05cf3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "class DownSample(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, negToPosRatio=2,  targetFeature='targetFeature', verbose=True):\n",
    "        self.negToPosRatio = negToPosRatio\n",
    "        self.targetFeature = targetFeature\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.print(\"=== DownSample ===\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        newX = X.copy()\n",
    "        neg = newX[newX[self.targetFeature]==0]\n",
    "        pos = newX[newX[self.targetFeature]==1]\n",
    "        neg = resample(neg, n_samples=len(pos)*self.negToPosRatio, random_state=0)\n",
    "        newX = pd.concat([pos, neg]).sort_index()\n",
    "        return newX\n",
    "\n",
    "    def print(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc8475",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dfTrainClean = pd.read_csv(\"clean_train.csv\", index_col='Unnamed: 0')\n",
    "assert dfTrainClean.index.is_monotonic_increasing\n",
    "assert dfTrainClean.indexingFeature.is_monotonic_increasing\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('EncodeDateTimeColumns', EncodeDateTimeColumns(patterns=[r\".*_dt.*\", r\".*_dttm.*\"], indexingFeature='indexingFeature', verbose=True)),\n",
    "    ('EncodeLowCardinality', EncodeLowCardinality(threshold=50, indexingFeature='indexingFeature', verbose=True)),\n",
    "    ('EncodeHighCardinality', EncodeHighCardinality(threshold=50,  targetFeature='targetFeature', indexingFeature='indexingFeature', verbose=True)),\n",
    "    ('DownSample', DownSample(negToPosRatio=2,  targetFeature='targetFeature', verbose=True))\n",
    "])\n",
    "\n",
    "dfTrainEnc = pipeline.fit_transform(dfTrainClean, dfTrain.idc_trgt)\n",
    "dfTrainEnc.to_csv(\"encoded_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1e5ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dfTestClean = pd.read_csv('clean_test.csv', index_col='Unnamed: 0')\n",
    "dfTrainClean = pd.read_csv(\"clean_train.csv\", index_col='Unnamed: 0')\n",
    "assert dfTestClean.index.is_monotonic_increasing\n",
    "assert dfTestClean.indexingFeature.is_monotonic_increasing\n",
    "\n",
    "testEncPipeline = Pipeline([\n",
    "    ('EncodeDateTimeColumns', EncodeDateTimeColumns(patterns=[r\".*_dt.*\", r\".*_dttm.*\"], indexingFeature='indexingFeature', verbose=True)),\n",
    "    ('EncodeLowCardinality', EncodeLowCardinality(threshold=50, indexingFeature='indexingFeature', parentDf=dfTrainClean, verbose=True)),\n",
    "    ('EncodeHighCardinality', EncodeHighCardinality(threshold=50,  targetFeature='targetFeature', indexingFeature='indexingFeature', parentDf=dfTrainClean, verbose=True))\n",
    "])\n",
    "dfTestEnc = testEncPipeline.fit_transform(dfTestClean, dfTestClean.idc_trgt)\n",
    "dfTestEnc.to_csv(\"encoded_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686def5",
   "metadata": {},
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87b033",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def time_series_split(df, target_variable='indexingFeature', n_splits=100, gap=100):\n",
    "    assert df.index.is_monotonic_increasing\n",
    "    assert df[target_variable].is_monotonic_increasing\n",
    "\n",
    "    newDf = df.copy()\n",
    "    newDf.index = range(len(newDf))\n",
    "    \n",
    "    current = 0\n",
    "    previous = 0\n",
    "    partitions = []\n",
    "    total = len(newDf.index)\n",
    "    indexOfVariableChanges = newDf.groupby(target_variable).apply(lambda group: group.index.max(), include_groups=False)\n",
    "\n",
    "    averageCount = (total-gap*(n_splits-1))/n_splits\n",
    "    requireGap = False\n",
    "    left = 0\n",
    "    right = None\n",
    "    for target, index in indexOfVariableChanges.items():\n",
    "        if index - current >= (averageCount if not requireGap  else gap):\n",
    "            previous = current\n",
    "            current = index\n",
    "            if requireGap:\n",
    "                requireGap = False\n",
    "                left = index+1\n",
    "            else:\n",
    "                requireGap = True\n",
    "                right = index\n",
    "                partitions.append((left, right))\n",
    "    return [(partitions[i][1], partitions[i+1]) for i in range(len(partitions)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26c74e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def miss_forest(df, startIndex=0, numCols=[], partitions=10, partition_variable='indexingFeature', rounds=5):\n",
    "    endIndex = max(df.index)\n",
    "    partitionRanges = time_series_split(df[startIndex:], target_variable=partition_variable, n_splits=10, gap=0)\n",
    "    X_new = df.copy()\n",
    "    X_new.drop(partition_variable, axis=1, inplace=True)\n",
    "    catCols = [col for col in X_new.columns if col not in numCols]\n",
    "    X_new.loc[:, numCols] = X_new[numCols].astype('float').fillna(X_new[numCols].mean())\n",
    "    X_new.loc[:, catCols] = X_new[catCols].fillna(X_new[catCols].mode().T.to_dict()[0])\n",
    "\n",
    "    round = 0\n",
    "    evals = {}\n",
    "    l = 0\n",
    "    k = df.loc[startIndex:].isna().sum().sort_values().index\n",
    "\n",
    "    while l <= 0 and round <= rounds:\n",
    "        evals[round] = {}\n",
    "        evals[round]['cat'] = {}\n",
    "        evals[round]['num'] = {}\n",
    "        lCatNum = 0\n",
    "        lCatDen = 1\n",
    "        lNumNum = 0\n",
    "        lNumDen = 1\n",
    "        X_old = X_new.copy()\n",
    "\n",
    "        for s in k:\n",
    "            if df[s].isna().sum() <= 0 or df[s].notna().sum() <= 0:\n",
    "                    continue\n",
    "            category = 'num' if s in numCols else 'cat'\n",
    "            print(f\"{s} ({category}): obs{df[s].notna().sum()} mis{df[s].isna().sum()}\")\n",
    "            obsCols = X_old.columns[X_old.columns != s]\n",
    "            obsIndex = df[df[s].notna()].index\n",
    "            misIndex = df[df[s].isna()].index\n",
    "            evals[round][category][s] = {}\n",
    "            evals[round][category][s]['base'] = []\n",
    "            evals[round][category][s]['impute'] = []\n",
    "            \n",
    "            for (trainLeftIndex, (testLeftIndex, testRightIndex)) in partitionRanges:\n",
    "                print(\".\", end=\"\")\n",
    "                i_obs = obsIndex[obsIndex <trainLeftIndex]\n",
    "                i_mis = misIndex[(misIndex <= testRightIndex) & (misIndex >= testLeftIndex)]\n",
    "                i_eval = obsIndex[(obsIndex <= testRightIndex) & (obsIndex >= testLeftIndex)]\n",
    "                assert df.loc[i_obs, s].isna().sum() == 0\n",
    "                assert df.loc[i_eval, s].isna().sum() == 0\n",
    "                assert df.loc[i_mis, s].isna().sum() == len(df.loc[i_mis, s])\n",
    "                \n",
    "                if len(i_obs) <= 0 or len(i_mis) <= 0:\n",
    "                    continue\n",
    "\n",
    "                if s in numCols:\n",
    "                    estimator = RandomForestRegressor(n_estimators=100, random_state=0, max_depth=5, oob_score=False, n_jobs=-1, warm_start=False)\n",
    "                else:\n",
    "                    estimator = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=5, oob_score=False, n_jobs=-1, warm_start=False)\n",
    "            \n",
    "                estimator.fit(X_old.loc[i_obs, obsCols], X_old.loc[i_obs, s])\n",
    "                X_new.loc[i_mis, s] = estimator.predict(X_old.loc[i_mis, obsCols])\n",
    "\n",
    "                eval_pred = estimator.predict(X_old.loc[i_eval, obsCols])\n",
    "\n",
    "                if s in numCols:\n",
    "                    lNumNum += sum((X_new.loc[i_mis, s]-X_old.loc[i_mis, s])**2/(X_old.loc[i_mis, s])**2)\n",
    "                    lNumDen += sum(X_old.loc[i_mis, s]**2)\n",
    "                    evals[round][category][s]['impute'].append(mean_absolute_error(X_old.loc[i_eval, s], eval_pred))\n",
    "                    evals[round][category][s]['base'].append(mean_absolute_error(X_old.loc[i_eval, s], [X_old.loc[i_obs, s].mean()]*len(X_old.loc[i_eval, s])))\n",
    "                else:\n",
    "                    lCatNum += sum(X_new.loc[i_mis, s]==X_old.loc[i_mis, s])\n",
    "                    lCatDen += len(i_mis)\n",
    "                    evals[round][category][s]['impute'].append(f1_score(X_old.loc[i_eval, s], eval_pred, average='macro'))\n",
    "                    evals[round][category][s]['base'].append(f1_score(X_old.loc[i_eval, s], [X_old.loc[i_obs, s].mode()[0]]*len(X_old.loc[i_eval, s]), average='macro'))\n",
    "            print(\":\")\n",
    "        l = (lCatNum/lCatDen)*(lNumNum/lNumDen)\n",
    "        round += 1\n",
    "    return X_new, evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f2481",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dfTrainEnc = pd.read_csv('encoded_train.csv', index_col='Unnamed: 0')\n",
    "dfTrainEncodedLeakFree = dfTrainEnc.drop(['idc_trgt'], axis=1)\n",
    "\n",
    "catCols = pipeline.named_steps['EncodeDateTimeColumns'].columns + pipeline.named_steps['EncodeLowCardinality'].columns + ['indexingFeature'] + dfTrainEnc.columns[dfTrainEnc.dtypes==np.dtype('bool')].tolist()\n",
    "numCols = [column for column in dfTrainEncodedLeakFree if column not in catCols]\n",
    "dfTrainImputed, evals = miss_forest(dfTrainEncodedLeakFree, startIndex=0, numCols=numCols, partitions=10, rounds=5)\n",
    "dfTrainImputed.drop(dfTrainImputed.columns[dfTrainImputed.isna().sum() > 0], axis=1, inplace=True)\n",
    "dfTrainImputed = dfTrainImputed.join(dfTrainEnc.idc_trgt, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d9439",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "x = np.nan_to_num([np.mean(x['impute']) for x in evals[0]['num'].values()], nan=0)\n",
    "y = np.nan_to_num([np.mean(y['base']) for y in evals[0]['num'].values()], nan=0)\n",
    "plt.plot(np.log(x+0.001), alpha=0.5, label='impute error')\n",
    "plt.plot(np.log(y+0.001), alpha=0.5, label='base error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67d9dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "x = np.nan_to_num([np.mean(x['impute']) for x in evals[0]['cat'].values()], nan=0)\n",
    "y = np.nan_to_num([np.mean(y['base']) for y in evals[0]['cat'].values()], nan=0)\n",
    "plt.plot(x, alpha=0.5, label='impute error')\n",
    "plt.plot(y, alpha=0.5, label='base error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2250e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dfTestEnc = pd.read_csv('encoded_test.csv', index_col='Unnamed: 0')\n",
    "dfTrainEnc = pd.read_csv('encoded_train.csv', index_col='Unnamed: 0')\n",
    "startIndex = min(dfTestEnc.index)\n",
    "combinedDf = pd.concat([dfTrainEnc, dfTestEnc]).drop(['idc_trgt'], axis=1)\n",
    "assert len(set(dfTrainEnc.index) & set(dfTestEnc.index)) == 0\n",
    "testIndex = dfTestEnc.index\n",
    "assert combinedDf.index.is_monotonic_increasing\n",
    "\n",
    "catCols = pipeline.named_steps['EncodeDateTimeColumns'].columns + pipeline.named_steps['EncodeLowCardinality'].columns + ['indexingFeature'] + dfTrainEnc.columns[dfTrainEnc.dtypes==np.dtype('bool')].tolist()\n",
    "numCols = [column for column in dfTrainEncodedLeakFree if column not in catCols]\n",
    "dfTestImputed, testEvals = miss_forest(combinedDf, startIndex=startIndex, numCols=numCols, partitions=10, rounds=5)\n",
    "dfTestImputed = dfTestImputed.loc[testIndex]\n",
    "dfTestImputed = dfTestImputed.join(dfTestEnc.idc_trgt, how='inner')\n",
    "dfTestImputed = dfTestImputed[dfTrainImputed.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada073b",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423f000",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "dfTrainImputed = pd.read_csv('imputed_train.csv')\n",
    "dfTrainImputed = dfTrainImputed.join(dfTrainClean.indexingFeature, how='inner')\n",
    "tscv = time_series_split(dfTrainImputed, target_variable='indexingFeature', n_splits=100, gap=100)\n",
    "dfTrainImputed.drop('indexingFeature', axis=1, inplace=True)\n",
    "dfTrainProcessedX = dfTrainImputed.drop('targetFeature', axis=1)\n",
    "dfTrainProcessedY = dfTrainImputed['targetFeature']\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=5, eval_metric='logloss', n_jobs=-1, nthread=4, random_state=0)\n",
    "\n",
    "tp = []\n",
    "fp = []\n",
    "fn = []\n",
    "tn = []\n",
    "shapValues = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for i, (trainLeftIndex, (testLeftIndex, testRightIndex)) in enumerate(tscv):   \n",
    "    trainX = dfTrainProcessedX.iloc[:trainLeftIndex+1]\n",
    "    trainY = dfTrainProcessedY.iloc[:trainLeftIndex+1]\n",
    "    testX = dfTrainProcessedX.iloc[testLeftIndex:testRightIndex+1]\n",
    "    testY = dfTrainProcessedY.iloc[testLeftIndex:testRightIndex+1]\n",
    "    assert len(set(trainX.join(dfTrainClean.indexingFeature, how='inner').indexingFeature) & set(testX.join(dfTrainClean.indexingFeature, how='inner').indexingFeature)) == 0\n",
    "    assert max(trainX.join(dfTrainClean.indexingFeature, how='inner').indexingFeature) < min(testX.join(dfTrainClean.indexingFeature, how='inner').indexingFeature)\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "    if len(trainY.unique()) < 2 or len(testY.unique()) < 2:\n",
    "            continue\n",
    "\n",
    "    model.fit(trainX, trainY)\n",
    "    y_pred = model.predict(testX)\n",
    "    cm=confusion_matrix(testY, y_pred)\n",
    "    tn.append(cm[0][0])\n",
    "    fp.append(cm[0][1])\n",
    "    fn.append(cm[1][0])\n",
    "    tp.append(cm[1][1])\n",
    "    precision, recall, thresholds = precision_recall_curve(testY,model.predict_proba(testX).T[-1])\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fpr, tpr, thresholds = roc_curve(testY,model.predict_proba(testX).T[-1])\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    \n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = shap.TreeExplainer(model).shap_values(testX)\n",
    "    shapValues.append(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4973500",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "for i in range(len(precisions)):\n",
    "    plt.plot(fprs[i], tprs[i], color='blue', alpha=0.1)\n",
    "    plt.plot(recalls[i], precisions[i], color='red', alpha=0.1)\n",
    "plt.xlabel(\"Recall/FPT\")\n",
    "plt.ylabel(\"Precision/TPR\")\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "line1 = Line2D([0], [0], label='fpr/tpr', color='blue')\n",
    "line2 = Line2D([0], [0], label='recall/precision', color='red')\n",
    "handles.extend([line1, line2])\n",
    "\n",
    "plt.legend(handles=handles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3a66a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tfMean = pd.merge(pd.DataFrame(pd.DataFrame(fprs).mean(), columns=['fpr']), pd.DataFrame(pd.DataFrame(tprs).mean(), columns=['tpr']), left_index=True, right_index=True).sort_values('fpr')\n",
    "tfStd = pd.merge(pd.DataFrame(pd.DataFrame(fprs).std(), columns=['fpr']), pd.DataFrame(pd.DataFrame(tprs).std(), columns=['tpr']), left_index=True, right_index=True).reindex(tfMean.index)\n",
    "\n",
    "plt.plot(tfMean.fpr, tfMean.tpr, color='blue', alpha=1, label='TF curve')\n",
    "plt.fill_between(tfMean.fpr, tfMean.tpr-tfStd.tpr, tfMean.tpr+tfStd.tpr, color='blue', alpha=0.2)\n",
    "\n",
    "prMean = pd.merge(pd.DataFrame(pd.DataFrame(recalls).mean(), columns=['recall']), pd.DataFrame(pd.DataFrame(precisions).mean(), columns=['precision']), left_index=True, right_index=True).sort_values('recall')\n",
    "prStd = pd.merge(pd.DataFrame(pd.DataFrame(recalls).std(), columns=['recall']), pd.DataFrame(pd.DataFrame(precisions).std(), columns=['precision']), left_index=True, right_index=True).reindex(prMean.index)\n",
    "\n",
    "plt.plot(prMean.recall, prMean.precision, color='red', alpha=1, label='PR curve')\n",
    "plt.fill_between(prMean.recall, prMean.precision-prStd.precision, prMean.precision+prStd.precision, color='red', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.ylabel('Precision/TPR')\n",
    "plt.xlabel('Recall/FPR')\n",
    "plt.title('Evaluation Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb14203",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "index = [i for i in range(len(tp))]\n",
    "accuracy = [(tp[i] + tn[i])/(tp[i] + tn[i] + fp[i] + fn[i]) for i in range(len(tp))]\n",
    "precision = [(tp[i])/(tp[i] + fp[i]) for i in range(len(tp))]\n",
    "recall = [(tp[i])/(tp[i] + fn[i]) for i in range(len(tp))]\n",
    "f1Score = [(2*precision[i]*recall[i])/(precision[i]+recall[i]) for i in range(len(tp))]\n",
    "specificity = [(tn[i])/(tn[i] + fp[i]) for i in range(len(tp))]\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.plot(index, accuracy, label = 'accuracy')\n",
    "plt.plot(index, precision, label = 'precision')\n",
    "plt.plot(index, recall, label = 'recall')\n",
    "plt.plot(index, f1Score, label = 'f1Score')\n",
    "plt.plot(index, specificity, label = 'specificity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919bf66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(f\"Accuracy: mean{np.mean(accuracy[i:])} sd{np.std(accuracy[i:])}\")\n",
    "print(f\"Precision: mean{np.mean(precision[i:])} sd{np.std(precision[i:])}\")\n",
    "print(f\"Recall: mean{np.mean(recall[i:])} sd{np.std(recall[i:])}\")\n",
    "print(f\"F1-score: mean{np.mean(f1Score[i:])} sd{np.std(f1Score[i:])}\")\n",
    "print(f\"Specificity: mean{np.mean(specificity[i:])} sd{np.std(specificity[i:])}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
