{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a network with labels on some nodes, how to assign labels to all other nodes in the network? Whereas before node-embedding was used; now an alternative framework, **message passing**, will be used. This will have nodes update their own idea of what their label is based on their neighbours' labels (similar to PageRank, etc.). This is based on the intuition that individual node behaviours are correlated in the network (nearby nodes will have the same label); due to homophily (tendency of individuals to associate and bond with similar others) and influence (social connection influence individual characteristics) dependencies of graphs.\n",
    "\n",
    "Based on thes correlations; classificaiton label of a node $v$ in the netwrok may depend on $v$'s features, the labels of $v$'s neighbours, the features of $v$'s neighbours (guilt-by-association).\n",
    "\n",
    "## Relational Classification\n",
    "Class probability $Y_v$ of node $v$ is a weighted average of class probabilities of its neighbours (unlabeled nodes initialize $Y_v=0.5$). After initialisation, update all nodes in a random order until convergence (or until maximum number of iterations reached). $$\\mathbb{P}(Y_v=c)=\\frac{1}{\\sum_{(v,u)\\in E}A_{v,u}}\\sum_{(v,u)\\in E}A_{v,u}\\mathbb{P}(Y_u=c)$$ If edges have strength/weight information $A_{v,u}$ (edge weight between $v$ and $u$). There are chanellenges with this model; namely that convergence is not guaranteed, and that the model cannot use node feature information.\n",
    "\n",
    "## Iterative Classification\n",
    "Building on relational classifier, iterative classification trains 2 classifiers; $\\phi_1(f_v)$ to predict node label based on node feature vector $f_v$, and $\\phi_2(f_v,z_v)$ to predict node label based on node feature vector $f_v$ and summary $z_v$ (histogram of each label, most common label, number of different labels, etc. in $N(v)$) of labels of $v's$ neighbours, $N(v)$. The architecture is then set up in two phases:\n",
    "\n",
    "1. **Classify based on node attributes alone**: On the training set, train classifiers (linear classifier, neural network, etc.)\n",
    "    - $\\phi_1(f_v)$ o predict $Y_v$ based on $f_v$\n",
    "    - $\\phi_2(f_v,z_v)$ o predict $Y_v$ based on $f_v$ and summary $z_v$ of labels of $v$'s neighbours.\n",
    "2. **Iterate until convergence**: On the test set, for each node in graph; initialise $Y_v$ based on the classifier $\\phi_1$, compute $z_v$, and predict the updated labels with $\\phi_2$. Iteratively re-compute $z_v$ and $Y_v$ (now only with $\\phi_2$) until class labels stabilize (or max iteration is reached).\n",
    "\n",
    "Here too, convergence is not guaranteed.\n",
    "\n",
    "## Belief Propogation\n",
    "Belief propogation is a dynamic programming approach to answer probability queries in a graph (probability of node $u$ belonging to class $1$); where iteratively neighbour nodes 'talk' and pass messages to each other. Once a consensus is reached, the final *belief* is calculated.\n",
    "This can be thought of formally as having a **label - label potential matrix** $\uppsi$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
