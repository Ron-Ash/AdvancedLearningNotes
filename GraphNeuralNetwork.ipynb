{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a network with labels on some nodes, how to assign labels to all other nodes in the network? Whereas before node-embedding was used; now an alternative framework, **message passing**, will be used. This will have nodes update their own idea of what their label is based on their neighbours' labels (similar to PageRank, etc.). This is based on the intuition that individual node behaviours are correlated in the network (nearby nodes will have the same label); due to homophily (tendency of individuals to associate and bond with similar others) and influence (social connection influence individual characteristics) dependencies of graphs.\n",
    "\n",
    "Based on thes correlations; classificaiton label of a node $v$ in the netwrok may depend on $v$'s features, the labels of $v$'s neighbours, the features of $v$'s neighbours (guilt-by-association).\n",
    "\n",
    "## Relational Classification\n",
    "Class probability $Y_v$ of node $v$ is a weighted average of class probabilities of its neighbours (unlabeled nodes initialize $Y_v=0.5$). After initialisation, update all nodes in a random order until convergence (or until maximum number of iterations reached). $$\\mathbb{P}(Y_v=c)=\\frac{1}{\\sum_{(v,u)\\in E}A_{v,u}}\\sum_{(v,u)\\in E}A_{v,u}\\mathbb{P}(Y_u=c)$$ If edges have strength/weight information $A_{v,u}$ (edge weight between $v$ and $u$). There are chanellenges with this model; namely that convergence is not guaranteed, and that the model cannot use node feature information.\n",
    "\n",
    "## Iterative Classification\n",
    "Building on relational classifier, iterative classification trains 2 classifiers; $\\phi_1(f_v)$ to predict node label based on node feature vector $f_v$, and $\\phi_2(f_v,z_v)$ to predict node label based on node feature vector $f_v$ and summary $z_v$ (histogram of each label, most common label, number of different labels, etc. in $N(v)$) of labels of $v's$ neighbours, $N(v)$. The architecture is then set up in two phases:\n",
    "\n",
    "1. **Classify based on node attributes alone**: On the training set, train classifiers (linear classifier, neural network, etc.)\n",
    "    - $\\phi_1(f_v)$ to predict $Y_v$ based on $f_v$\n",
    "    - $\\phi_2(f_v,z_v)$ to predict $Y_v$ based on $f_v$ and summary $z_v$ of labels of $v$'s neighbours.\n",
    "2. **Iterate until convergence**: On the test set, for each node in graph; initialise $Y_v$ based on the classifier $\\phi_1$, compute $z_v$, and predict the updated labels with $\\phi_2$. Iteratively re-compute $z_v$ and $Y_v$ (now only with $\\phi_2$) until class labels stabilize (or max iteration is reached).\n",
    "\n",
    "Here too, convergence is not guaranteed.\n",
    "\n",
    "## Belief Propogation\n",
    "Belief propogation is a dynamic programming approach to answer probability queries in a graph (probability of node $u$ belonging to class $1$); where iteratively neighbour nodes 'talk' and pass messages to each other. Once a consensus is reached, the final *belief* is calculated.\n",
    "This can be formally defined as $$m_{i\\rightarrow j}(Y_j) = \\sum_{Y_i\\in\\mathcal{L}}\\psi(Y_i,Y_j)\\phi(Y_i) \\prod_{k\\in N(i) / \\{j\\}} m_{k\\rightarrow i}(Y_i),\\;\\forall Y_j \\in\\mathcal{L}$$ where the **label-label potential matrix**, $\\psi$, captures the dependencies between a node and its neighbour ($\\psi(Y_i,Y_j)$ is proportional to the probability of a node $j$ being in class $Y_j$ given that it has neighbour $i$ in class $Y_i$ - if graph is homophily then will have high values on the diagonal), $\\phi(Y_i)$ (**Prior belief**) is proportional to the probability of node $i$ being in class $Y_i$, $m_{i\\rightarrow j}(Y_j)$ being node $i$'s message/estimate of $j$ being in class $Y_j$; and $\\mathcal{L}$ is the set of all classes.\n",
    "This iterative calculation (with initialized messages being $1$) will eventually converge, and node $i$'s belief of being in class $Y_i$ could then be calculated via $b_i(Y_i)=\\phi(Y_i)\\prod_{j\\in N(i)} m_{j\\rightarrow i}(Y_j),\\;\\forall Y_j \\in\\mathcal{L}$.\n\n",
    "This process (in practice) is still run even when the graph is not acyclical (messages from different subgraph are no longer independent); though it will may mean that beliefs will not converge (loops can reinforce incorrect prior beliefs).\n",
    "\n",
    "# Graph Neural Network (GNN)\n",
    "Assuming a graph $G$, with vertex set $V$, adjacency matrix $A$ (binary), node features $X\\in\\mathbb{R}^{m}\\times\\mathbb{R}^{|V|}$, and node neighbourhood $N(v),\\;v\\in V$; a naive approach would be to concatonate $A$ and $X$ together and feed it straight into a deep neural network. This however, is sensitive to node ordering, is not applicable to graphs of different sizes, and would require learning $O(|V|)$ parameters; thereby is not an ideal solution.\n\nUsing the prespective of an image being a strict lattice graph, some sort of (generalized) convolutional neural network may be designed to capture a general graph instead. With a traditional convolution layer, there is no fixed notion of locality (window) or transition (sliding) for a generic graph; also, as a graph does not have a canonical ordering of nodes it means that convolutional neural networks will not be a *permutation invariant/equivariant* function:\n",
    "- A graph function $f:(\\mathbb{R}^{|V|}\\times\\mathbb{R}^{|V|}, \\mathbb{R}^{|V|}\\times\\mathbb{R}^{m})\\rightarrow \\mathbb{R}^{d}$ is ***permutation invariant*** if $f(A,X)=f(PAP^T,PX)$ for any permutation $P$.\n",
    "- A **node** function $f:(\\mathbb{R}^{|V|}\\times\\mathbb{R}^{|V|}, \\mathbb{R}^{|V|}\\times\\mathbb{R}^{m})\\rightarrow \\mathbb{R}^{|V|}\\times\\mathbb{R}^{d}$ is ***permutation equivariant*** if $Pf(A,X)=f(PAP^T,PX)$ for any permutation $P$.\n",
    "So instead of a sliding window, a convolutional layer is thought of as combining transformed neighbour information $h_i = \\sum_{j \\in N(i)}w_jh_j$, and doing so iteratively across the whole graph. A GNN *convolutional layer* (node embedding) can then be defined by determining a node of interest's **computation graph** (local network neighbourhood) and propograte/transform information from it back to compute the node features (embedding).\n",
    "Doing so would generate a unique computation graph for each node $v$, with nodes having embeddings at each layer such that layer $k$ embeddings gets information from $N_k(v)$ (nodes which are $k$ hops away such that $i\\subseteq N_v(k-1)\\rightarrow N_1(i)\\subseteq N_v(k)$). These neighbourhood embeddings ($N_1(i)$) are then aggregated at each layer transition, $N_1(i)\\subseteq N_v(k)\\xrightarrow{f} i'$ (aggregation operator $f$ needs to be permutation invariant), before being passed through a **node-unique** tranformation operator $i'\\xrightarrow{\\mathcal{n}_{v,i}^{(k)}}i\\subseteq N_v(k-1)$.\n",
    "The basic approach would then be to average the neighbour messages (aggregation operator) and apply a neural network (transformation operator): $$h_{i}^{(k-1)}=\\sigma(W^{(k)}\\sum_{j\\in N_1(i)}\\frac{h_j^{(k)}}{|N_1(i)|}+B^{(k)}h_i^{(k)}),\\;i\\in N_{k-1}(v),\\;h_i^{K}=\\text{x}_i,\\;\\;\\forall k\\in\\{K,\\ldots,0\\}$$where the initial embedding $\text{x}_v^{(K)}$ is node $v$'s features, and the trainable parameters $W^{(k)}$ and $B^{(k)}$ are shared across the layer (not unique to $h_{i}^{(k)}$). These embeddings can be then fed into any loss function and be optimised through SGD.\n",
    "- This additionally means that the GCB computation is permutation equivariant(embedding of a given node with GCN is invariant and so after permutation, the location of a given node in input node feature matrix would change but maintain its embedding value).\n",
    "Furthermore, these aggregation and traqnsformation can be performed efficiently by (sparse) matrix operations: $$H^{(k)}=[h_1^{(k)},\\ldots,h_{|V|}^{(k)}]^T\\rightarrow\\sum_{u\\in N_1(v)}h_u^{(k)}=A_{(v,\\,:)}H^{(k)}$$$$D\\in\\mathbb{R}^{|V|}\\times\\mathbb{R}^{|V|}:D_{(v,v)}=d_v=|N_1(v)|\\rightarrow D^{-1}\\in\\mathbb{R}^{|V|}\\times\\mathbb{R}^{|V|}:D^{-1}_{(v,v)}=1/|N_1(v)|$$$$\\therefore \\sum_{u\\in N_1(v)}\\frac{h_u^{(k)}}{|N(v)|}=D^{-1}AH^{(k)},\\;H^{(k-1)}=\\sigma(\\underbrace{\\color{red} D^{-1}AH^{(k)}W_k^T \\color{black}}_{\\text{neighbour aggregation}} \\overbrace{\\color{blue} H^{(k)}B_k^T \\color{black}}^{\\text{self transform}}))$$\n",
    "In practice, this implies that efficient sparse matrix multiplication can be used ($\\tilde{A}=D^{-1}A$ is sparse). However, not all GNNs can eb expressed in a simple matrix form (when aggregation function is complex).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
