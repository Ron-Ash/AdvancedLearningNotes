# Representational Learning
Whereas traditional feature engineering required hand-made features to be created for the downstream prediction task; representational learning attempts to achieve efficient, task-independent, automatic, feature learning for machine learning with graphs. The idea is to map an aspect of the graph into a $d$-dimensional vector (embedding space), capturing the underlying structure.

![image.png](attachment:image.png)

This embedding space is suppose to reflect the original aspects themselves, thereby similarities within the embeddings indicate similarity in the original structures.

## Node Embedding
For node-level tasks, with the goal of encoding nodes into an embedding space such that their similarity, evaluated by dot product, approximate similarity in teh graph; the **Encoder Decoder Framework** is used.
![image-2.png](attachment:image-2.png)
1. Encoder, $\text{ENC}:(u)\rightarrow \mathbb{R}^d=z_u$, maps from nodes to embeddings
2. Node similarity function, $\text{similarity}:(u,v)\rightarrow\mathbb{R}$, measures the similarity between nodes in the original network.
3. Decoder, $\text{DEC}:(z_u,z_v)\rightarrow\mathbb{R}$, maps embeddings to teh similarity score (most commonly $\text{DEC}(z_u,z_v)=z_u^Tz_v$).
4. Optimize the parameters of the encoder such that $\text{similarity}(u,v)\approx\text{DEC}(z_u,z_v)$

### Shallow Encoding
The simplest encoding approach is an "embedding-lookup", $\text{ENC}(u)=z_u = Z\cdot \mathcal{V}_u,\;Z\in\mathbb{R}^d\times\mathbb{R}^{|V|},\;\mathcal{V}_u\in\text{indicator}^{|V|}$, where a large matrix is learned where column $i$ corresponds to the $i^{th}$ node embedding ($\mathcal{V}_i$ is then simply a vector with all zeros except in the $i^{th}$ element where is it 1).
![image-3.png](attachment:image-3.png)

Note that this approach is not very scalable as $Z$ increases with the number of nodes in the network.

#### DeepWalk
This choice of encoder relies on the idea of **random walks** (from an initial node $u$, using some randomising algorithm move to neighbouring nodes, and after $l$ iterations record the path taken) being an (un/self-)supervised way of learning the node embeddings (task independent as not labels/features are utilized) which preserves network structures within the embedding (local and higher-order neighbourhood information), and does not require all node pairs to be considered (only neeed to consider pairs that co-occur on random walks).

By first defining the similarity function as "the probability that nodes $u$ and $v$ co-occur on a random walk over the graph", and the decoder as $z_u^Tz_v$; an embedding can be created by estimating the probability of visiting node $v$ on a random walk starting from node $u$ using some random algorithm $R$, $\mathbb{P}(v|u)$, and optimizing the embedding to encode these random statistics towards $z_u^Tz_v$.

Given $G=(V,E)$, the mapping function $f:u\rightarrow\mathbb{R}^d=z_u$ can be learned through optimizing (log-likelihood objective) $\max_f\sum_{u\in V}\log\mathbb{P}(n_R(u)|z_u)$, where $N_R(u)$ is the neighbourhood of node $u$ by strategy $R$:
1. run short fixed-length randowm walks starting at each node $u$ in teh graph using some strategy $R$
2. for each node $u$ collect the $N_R(u)$ multiset (keep repeated elements if visited multiple times)
3. optimise embedding according to $\max_f\sum_{u\in V}\log\mathbb{P}(n_R(u)|z_u)$ or equivalently minimise $\sum_{u\in V}\sum_{v\in N_R(u)}-\log\mathbb{P}(v|z_u)$
    - $\mathbb{P}(v|z_u)$ can be parameterized using the softmax equation (function turning vector of $K$ real values into $K$ probabilities that sum to 1) $\sigma(z)_i=\frac{\exp{z_i}}{\sum_{j=1}^{K}\exp{z_j}}$, $\mathbb{P}(v|z_u)=\frac{\exp{z_u^Tz_v}}{\sum_{n\in V}\exp{z_u^Tz_n}}$

Note that his is expensive as the optimisation includes a nested sum, resulting in a $O(|V|^2)$ complexity. This can be resolved using **negative sampling**, where istead of normalizing with respect ot all nodes, it is normalized against $k$ (in practice $k\in [5,20]$) random "negative sample" (each with probability proportional to its degree):$$\log(\frac{\exp{z_u^Tz_v}}{\sum_{n\in V}\exp{z_u^Tz_n}})\approx \log(S(z_u^Ts_v))-\sum_{i=1}^{k}\log(S(z_u^Tz_i))$$
- higher $k$ gives more robust estimates, and corresponds to a higher bias on negative events.

After the objective function has been obtained, stochastic gradient descent can be used to optimize it.

##### node2vec
This alteration to deep walk utilises a different $R$ definition based on the observation that a flexible notation of network neighbourhood $N_R(u)$ leads to rich node embeddings.
![image-4.png](attachment:image-4.png)
Thereby, a biased random walk that can trade off between local (Bridth First Search) and global (Depth First Search) views of the network can be achieved via the addition of 2 paramerters to $R$; return parameter $p$ (return back to the previous node), and in-out parameter $q$ (the "ratio" of BFS v DFS affinity).
![image-5.png](attachment:image-5.png)
If a random walk just traversed edge $(t,w)$ and is now at $w$, edge transission probabilities (un-normalized) can be defined by the new parameters:
- transition **back** to $t$ is set to $1/p$ (low $p$ for BFS-like walk)
- transition to $s_1$ which is at the **same distance** from $t$ is set to $1$
- transition **away** from $t$ to $s_2$ is set to $1/q$ (low $q$ for DFS-like walk)
